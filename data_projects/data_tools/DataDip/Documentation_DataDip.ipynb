{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataDip Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this documentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a guide to using the DataDip jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is DataDip?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataDip is a jupyter notebook using Pandas and Matplotlib. The code is organized in a way to facilitate the process of doing basic data analyses with a dataset. DataDip contains many standard processes for inspecting, cleaning, manipulating, and plotting a dataset. This is a tool that offers data analysis functionalities at no cost and requiring little to no programming, yet, allows the user to reprogram if they choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do I need to use DataDip?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use DataDip, you need a method to use Jupyter Notebooks. The original version was developed in VSCode and is optimized for use in that code editor. Since DataDip is a rather long notebook with many sections, having a tool that gives the outline of the notebook with autoscroll to sections will be helpful (VS Code has this).\n",
    "\n",
    "Some knowledge of Python, Pandas, Matplotlib and Jupyter Notebooks (ipynb) would be beneifical but the amount needed can be easily picked up while learning to use this tool. Specifically know how to\n",
    "\n",
    "1. Run cells in ipynb\n",
    "2. single and multiline commenting in python\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does DataDip generally work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataDip allows for\n",
    "\n",
    "1. importing dataset and libraries, and defining constants\n",
    "2. inspecting data\n",
    "3. cleaning data\n",
    "4. manipulating data and performing calculations\n",
    "5. plotting data\n",
    "\n",
    "To implement the above behaviors, the notebook is organized in the following way\n",
    "\n",
    "1. In/Out Setter\n",
    "2. Prelims\n",
    "3. Glance\n",
    "4. Clean\n",
    "5. Functions\n",
    "6. Calculations\n",
    "7. DF Creation\n",
    "8. Plot\n",
    "9. Custom\n",
    "\n",
    "Each section is described in detail its respective section in this documentation. Each section can be collapsed or opened. If using a code editor such as VS Code, use outline to jump to sections. Sections may have subsections which help organize the processes further. Subsections are prepended with '`', subsubsection with double ticks, '``', and so on. \n",
    "\n",
    "There is an important process that is generally applied across the notebook, the **In/Out Method** (see below).\n",
    "\n",
    "Furthermore, most cells are collapsed by default, with many not requiring to be opened to be properly run. Experienced users can glance at the cell to understand quickly if any modifications are required. Inexperienced users can read the respective section in this documentation to see if they must modify aspects of the function before running the cell or can simply just run the cell. The documentation will usually not express need to not alter a cell and will express need to have to alter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The In/Out Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 important variables in DataDip.\n",
    "1. 'df'\n",
    "2. 'df_altered_in'\n",
    "3. 'df_altered_out'\n",
    "\n",
    "The imported dataset is assigned to 'df'. However, 'df' is left unaltered so the user can always use the original dataframe (df) without having to reimport.\n",
    "\n",
    "'df_altered_in' is the main version of the df that the user will output alterations to. This df is to be used as the current saved version of the 'df' after alterations. Most processing of the df will take in 'df_altered_in' and output the result to 'df_altered_out'. A user can then inspect 'df_altered_out' and confirm modifications before saving it to 'df_altered_in'. The user can also saved 'df_altered_out' to a variable of their chosing before saving that back to 'df_altered_out' and then to 'df_altered_in' (or do so directly). The user saves any 'df_altered_out' (or other df) to 'df_altered_in' with the In/Out Setter cell which is the top most cell of the notebook. \n",
    "\n",
    "This process will hopefully create a intermediary state of the df to prevent the user from unintentionally affecting in undesirable ways the current state of the df, i.e. 'df_altered_in'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sections will have a cell that assigns the 'df_altered_in' to the df used for that section, e.g. in the Filerting section we see \n",
    "\n",
    "df_to_filter = df_altered_in\n",
    "\n",
    "The section df is then used for the processes within the section. The output of the section processes overwrites 'df_altered_out'. All this is setup this way so that the user can select either the 'df_altered_in' or another df as the input for any of the subsection processes. With the output being 'df_altered_out', the user can inspect the output before saving it to 'df_altered_in'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In/Out Setter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a single cell section. It is the top most cell of the notebook:\n",
    "\n",
    "df_altered_in = df_altered_out\n",
    "\n",
    "This cell is used to save any altered version of the df. 'df_altered_in' is typically used as the input df for most processe in DataDip, while 'df_altered_out' is used for the output df of most processes.\n",
    "\n",
    "Once the user is content with the state of 'df_altered_out' after modifications, the can quickly run the In/Out Setter to save that to the main saved state of the df.\n",
    "\n",
    "*Note: recall that 'df' is saved for the original, unaltered version of the df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the prelimnary section which allows for importing libraries, loading data into a df, and defining constants.\n",
    "\n",
    "Import cell doesn't need to be modified before running, unless specifics are required.\n",
    "\n",
    "The second cell is used for loading data. The user must define the file path for the dataset before running cell. By default, the input file type is csv. For other file formats comment the read_csv method and uncomment the version desired in the same cell.\n",
    "\n",
    "'df' is reserved for the origianl, unaltered df. 'df' will be saved to 'df_altered_in' when running the second cell.\n",
    "\n",
    "The third cell is for defining constants. Empty by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Glance section is used for visualizing the df, determining some important aspects of the df (like which columns have nulls), and getting basic statistics of the df. It is split into 3 sections: display, table aspects, and stats.\n",
    "\n",
    "Glance processes use 'df_to_glance' = 'df_altered_in'\n",
    "\n",
    "Run Glance header first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run enclosed cell to view head and tail of 'df_to_glance'.\n",
    "\n",
    "In vscode, to view data in separate tab, open variables view (click 'varaibles' button at top of editor). Then next to df, click the \"show variable snapshot in dataviewer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Table Aspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions\n",
    "1. Get column names\n",
    "    - returns a list of all column names\n",
    "2. Get column with null values\n",
    "    - returns a list of all columns containing nulls\n",
    "3. Get unique rows of column(s)\n",
    "    - find the unique values of any or all columns. This cell may require modification to run properly. By default it will return a dictionary of the unique values of all columns. Use the list version in cell for unique columns.\n",
    "4. Types Checker\n",
    "    - checks to see if there are mixed types in all columns. Returns True for mixed, False for not mixed\n",
    "    - returns the type(s) in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions\n",
    "1. df.describe()\n",
    "    - returns count, mean, std, min, max, 1st quartile max value (25%), 2nd quartile max value aka median (50%), 3rd quartile value (75%)\n",
    "2. mode(s)\n",
    "    - returns mode(s)\n",
    "    - by default will get mode for all columns\n",
    "    - can select for a selection of columns or can select to remove columns. Uncomment and comment as necessary\n",
    "3. sum(s)\n",
    "    - returns sum(s)\n",
    "    - by default will get sum for all columns\n",
    "    - can select for a selection of columns or can select to remove columns. Uncomment and comment as necessary\n",
    "3. var(s)\n",
    "    - returns variance(s)\n",
    "    - by default will get variance for all columns\n",
    "    - can select for a selection of columns or can select to remove columns. Uncomment and comment as necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Header assigns 'df_to_clean'\n",
    "\n",
    "You can remove null rows from all columns. Doesn't require modifications.\n",
    "\n",
    "You can remove null rows from specific columns. Requires modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes various data manipulation functions. Each type of function will have subsets of its behavior. The df used in each function is specified in a header cell in each function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Header cell assigns: df_to_filter = df_altered_in\n",
    "\n",
    "There are several types of filtering methods. Each filtering type has a cell for the function and a cell for the argument definitions. Before running the argument cell, run the function cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ``Main Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process**\n",
    "\n",
    "The main filter function, filter_main(), has the behavior of taking in a dataframe and outputting a filtered_df based on the arguments passed to it. It takes in 2 arguments.\n",
    "1. df_to_filter\n",
    "    - this is the df that is to be filtered based on the conditions\n",
    "2. conditions\n",
    "    - this argument contains the column name(s) and condition(s).\n",
    "    - this is the part that determines which columns and conditions to filter the database on\n",
    "    - the string must be in the following form, with column_name and condition within parantheses and parantheses groups connected with logical operators (LO), if any. E.g.\n",
    "        - For single condition:\n",
    "            - '(column_name > condition_1)'\n",
    "            - e.g. '(Age > 30)'\n",
    "        - For multiple conditions:\n",
    "            - '(column_name > condition_1) LO (column_name > condition_1)'\n",
    "            - '(Age > 30) & (Gender == \"Male\")'\n",
    "\n",
    "For more information on operators such as >, ==, &, |, etc, see **Operators Key** below\n",
    "\n",
    "\n",
    "**Operators Key**\n",
    "\n",
    "use any of the following operators for the filtering condition\n",
    "\n",
    "1. Comparison Operators\n",
    "    - ==, !=, >, <, >=, <=\n",
    "    - e.g. '(Age > 30)'\n",
    "    - note: '==' can be used for strings but checks for exact equality. To search for a substring within a string, try filter_string().\n",
    "2. Logical Operators\n",
    "    - '&' (and)\n",
    "    - '|' (or)\n",
    "    - '~' (not)\n",
    "    - e.g. '(Age > 30) & (Gender == \"Male\")'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ``String Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string filter, filter_string(), takes in 3 arguments. The difference between filter_string() and filter_main() is that filter_string() can be used to find substrings within string columns, whereas filter_main() will find exact equivalent string.\n",
    "1. df_to_stringFilter\n",
    "    - df to be filtered with string condition\n",
    "2. conditions_dict\n",
    "    - dictionary with key=column_name and value=substring used for passing the column_name and substring to be searched for. the dictionary can have >=1 key,value pairs\n",
    "    -e.g. {'Name': 'John', 'Last_Name': 'Doe', }\n",
    "3. case_sensitive\n",
    "    - case_sensitive = False, then the search is case insensitive\n",
    "    - case_sensitive = True, then the search is case sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ``List Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list filter, filter_by_isin(), takes in 3 arguments. This filter is used to filter rows based on a list. This function allows the user to pass 1 or more conditions and allows for filtering by the negation of the list.\n",
    "\n",
    "Arguments:\n",
    "1. df_to_list_filter\n",
    "    - df to be filtered with list condition\n",
    "2. conditions_dict\n",
    "    - dictionary with key='column_name' and value=[list of strings] \n",
    "    - these are the columns and strings to which the isin() method will filter by. \n",
    "    - the dictionary can have >=1 key,value pairs\n",
    "    - e.g. {'Category': ['Fruit', 'Vegetable'], 'Color': ['Red', 'Green']}\n",
    "3. negate\n",
    "    - when set to True, this will find the not of the conditions\n",
    "    - e.g. with {'Category': ['Fruit', 'Vegetable'], 'Color': ['Red', 'Green']}, we'd have not 'Fruit', not 'Vegetable', not 'Red', and not 'Green'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ``Null Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null filter, filter_by_nulls(), takes in 3 arguments. This filter is used to filter rows based on whether rows are null or not null.\n",
    "\n",
    "Arguments:\n",
    "1. df_to_list_filter\n",
    "    - df to be filtered with list condition\n",
    "2. columns_to_null_filter\n",
    "    - list of the columns to check for the condition\n",
    "    - e.g. ['column_1', 'column_2']\n",
    "3. is_null\n",
    "    - when set to True, this will find the rows that are null, set to False, rows that are not null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Header: df_to_sort\n",
    "\n",
    "1. Sort by Columns\n",
    "    - there are two cells. 1. function cell, 2. argument cell. Run the function cell prior to running argument cell. Argument cell requires modifications.\n",
    "    - can sort by single column or by multiple columns. in both cases, the sorting is extended to all values in the row to maintain correspondence.\n",
    "    - the function takes in an argument called sorting_criteria\n",
    "        - sorting_criteria is a dictionary with key being column name and value being True or False indicating ascending or descending respectively\n",
    "        - if only one column, just have one key,value pair\n",
    "        - if more than one column, comma separate key,value pairs\n",
    "            - for more than one column the behavior is as follows. The sorting mechanism will sort according to the first sorting_criteria key,value pair. if there is a tie with that one, then it will progress to the next sorting_criteria key,value piar and sort accordingly. this continues until there is no tie.\n",
    "2. Sory by Index\n",
    "    - this is a single cell that will sort the 'df_to_sort' df by index only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is used for custom calculations. Requires modifcations.\n",
    "\n",
    "The overall behavior is that you can perform calcuations on any number of rows and have the output to a new column.\n",
    "\n",
    "First assign 'new_column' the name you want for the output columns. Then modify the custom_calculation() function to suit your needs. Run cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section allows you to create new dfs based on certain general conditions. All except .copy() will require some modifications of the cells before running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Plot section contains standard plot options. These cells require you to determine the column(s) to plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
